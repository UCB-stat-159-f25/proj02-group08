# Project 2: Reproducibility in Natural Langauge Processing

* **Statistics 159/259, Fall 2025**
* **Due 11/21/2025, 11:59PM PT**
* Prof. F. PÃ©rez, GSI J. Butler, and GSI S. Andrade, Department of Statistics, UC Berkeley.
* Your score for this assignment is out of **70 points** (with a potential of 7 points extra credit). This project as a whole accounts for **20% of the final course grade**.
* Assignment type: **group project assignment**

The assignment content is located in `proj02-nlp.ipynb`.

## Project Description TODOs

**Learning Objectives:**
- Implement Modern NLP Pipelines: 
    - Load and Prepare Data: Ingest and clean a real-world text dataset using pandas.
        - [ ] #todo-01 ...
     
    - Process Text Efficiently: Implement a text pre-processing pipeline using spaCy, a production-grade NLP library.
        - [ ] #todo-01 ...
     
    - Understand Core Concepts: Understand the diffence between a token, a lemma, a stop word, and punctuation, and understand why lemmatization was preferred for text analysis prior to LLMs.
        - [ ] #todo-01 ...

  
- Conduct Core Text Analysis:
    - Extract Linguistic Features: Use spaCy to efficiently extract lemmas, parts-of-speech, and named entities from raw text.
        - [ ] #todo-01 ...

    - Perform Frequency Analysis: Use spaCy outputs to compare the most frequent words in different documents.
        - [ ] #todo-01 ...

    - Analyze Text Over Time: Compare the language of modern presidents to historical ones, drawing initial conclusions about how political communication has evolved.
        - [ ] #todo-01 ...

- Build and Compare Topic Models:
    - Vectorize Text: Understand and apply the TF-IDF (Term Frequency-Inverse Document Frequency) vectorization method to prepare text for machine learning.
        - [ ] #todo-01 ...
  
    - Implement Traditional Topic Modeling: Build, train, and interpret a Latent Dirichlet Allocation (LDA) topic model using gensim to find topics based on word co-occurrence.
        - [ ] #todo-01 ...
     
    - Implement Modern Topic Modeling: Use BERTopic to leverage transformer embeddings (BERT), clustering, and semantic similarity to discover conceptually coherent topics.
        - [ ] #todo-01 ...
  
    - Critically Evaluate Models: Compare the topics generated by LDA and BERTopic, analyzing the trade-offs and advantages of each approach (i.e., "bag-of-words" vs. "semantic similarity").
        - [ ] #todo-01 ...


- Reproducibility and Collaboration:
    - Work on more open-ended questions that involve self learning, utilization of external resources, and exploration
        - [ ] #todo-01 ...
  
    - Collaboration on Complex Projects: Gain experience coordinating on teams to complete tasks with dependencies on each other
        - [ ] #todo-01 ...
  
    - Reproducibility: Gain experience describing your work in accessibile formats and making it reproducible.
        - [ ] #todo-01 ...


**Deliverables:** For this assignment, you will have a single GitHub repository for your group. Your repository should contain the following:

- One notebook for each of Parts 1, 2, and 3 in the assignment notebook `proj02-nlp.ipynb` (and Part 4 if you wish to do it) that includes code to create the plots and simulations the question asks for, along with any written responses, commentary, discussion, and documentation where applicable. Please name each notebook using the convention `nlp-PXX.ipynb`, with `XX` corresponding to the number of the part. Please remember to use markdown headings for each section/subsection so the entire notebook document is readable. All figures should be both rendered in the notebook and saved in a separate folder called `outputs`. *Please also make sure to structure your notebooks as if you were conducting this as a clean and nicely presented data analysis report. Do not include our prompts/problem statements in the final report notebooks.*
    - [ ] #todo-01 Create `nlp-P01.ipynb` for Part 01 of `proj02-nlp.ipynb`
    - [ ] #todo-02 Create `nlp-P02.ipynb` for Part 02 of `proj02-nlp.ipynb`
    - [ ] #todo-03 Create `nlp-P03.ipynb` for Part 03 of `proj02-nlp.ipynb`
    - [ ] #todo-04 Create `nlp-P04.ipynb` for Part 04 of `proj02-nlp.ipynb` - Optional
    - [ ] #todo-05 Use markdown headings for sections and subsections to maintain readability
    - [ ] #todo-06 Render figures in the notebook and save figures to `outputs/` directory
    - [ ] #todo-07 Structure notebooks as professional data anaysis reports i.e. clean and nicely formatted for presentation and reporting
    - [ ] #todo-08 Do not include your prompts/problem statements in the final report notebooks

- Complete the contribution statement in `contribution_statement.md`, briefly and qualitatively detailing each group member's contributions to the assignment.
    - [ ] #todo-01 Create and complete the `contribution_statement.md`
    - [ ] #todo-02 Breifly and quantitatively detailing each group member's contribution

- An `ai_documentation.txt` file where your group will put any prompts and output from AI companions.
    - [ ] #todo-01 Create and complete the `ai_documentation.txt`
    - [ ] #todo-02 Log any and all prompts and outputs from AI companions

- A README describing the aims of the project, with a Binder link to the repo so your notebooks can be run.
    - [ ] #todo-01 Add `README.md` file to the project
    - [ ] #todo-02 Add a binder link to the `README.md`

- A MyST Site for your project, deployed to GitHub Pages. Each notebook should have its own tab on the website.
    - [ ] #todo-01 Add MyST site to the project
    - [ ] #todo-02 Ensure each notebook has a tab on the website
 

There are two core components to this homework:

1. Working with text data building from simple to complex methods (Parts 1-4).
2. Building a reproducible and structured repository (Part 5).

We have provided you with detailed questions with hints and will cover similar material in the lab.

The total grade for this homework is divided between:
- [15 Points] Part 1: Data loading and Initial Exploration.
    - [ ] #part-01-done ...
 
- [20 Points] Part 2: Simple Text Processing - Tokenization, Lemmatization, Word Frequency, Vectorization.
    - [ ] #part-02-done ...
 
- [20 Points] Part 3:  Advanced Text Processing - LDA and BERTopic Topic Modeling
    - [ ] #part-03-done ...
 
- [Extra Credit: 7 Points] Part 4: Choose your own adventure!
    - [ ] #part-04-done ...
 
- [15 Points] Part 5: Project Structure
    - [ ] #part-05-done ...

Parts 1-4 will each be graded from their respective notebooks. Part 5 will be graded by assessing whether all deliverables mentioned above are present, as well as your team's use of git (e.g. effective commit messages, use of .gitignore for handling extraneous/unnecessary files, repo organization, etc.)

When running your analyses and making plots, it may be useful to save intermediate steps, such as vectorized data files and then read them in order to make just the plots.

**Note: The parts successively build on each other, so you will need to strategize how to divde and conquer in your groups. For example, some topic models from part 3 require the vectorization used in part 2.**

**Acknowledgment:** This homework assignment has been revamped from the previous Spring 2017 assignment created by Prof Fernando Perez and Eli Ben-Michael, adapted from Prof Deb Nolan previous version of this project. The data from the project is from a Kaggle dataset here: https://www.kaggle.com/datasets/nicholasheyerdahl/state-of-the-union-address-texts-1790-2024?resource=download 


---

# âœ… STAT 159/259A â€” Project 02 Checklist
**Project:** Reproducibility in Natural Language Processing  
**Course:** Statistics 159/259 â€¢ Fall 2025  
**Due:** **Nov 21 2025 @ 11:59 PM PT**  
**Team:** Group 08  
**Lead:** Graduate Student (You)

---

## ðŸ§­ Section 1 â€” Repository Setup & Structure (Part 5 = 15 pts)

- [ ] Repo exists and is public under course org (`proj02-group08`)
- [ ] All team members have write access
- [ ] Clean directory layout (`data/`, `notebooks/`, `outputs/`, `report/`, `docs/`)
- [ ] `.gitignore` excludes large data files and cache
- [ ] `README.md` present â€” overview + Binder badge + run instructions
- [ ] `environment.yml` or `Pipfile` runs on DataHub (spaCy, gensim, BERTopic, pandas, sklearn)
- [ ] `Makefile` (or script) automates `make all`
- [ ] MyST site deployed on GitHub Pages (each notebook = one tab)
- [ ] `ai_documentation.txt` added (prompts + outputs)
- [ ] `contribution_statement.md` completed with roles and tasks

---

## ðŸ“Š Section 2 â€” Part 1 (15 pts): Data Loading & Exploration

- [ ] Load Kaggle dataset (State of the Union 1790 â€“ 2024)
- [ ] Inspect structure (number of docs, years, presidents)
- [ ] Descriptive EDA (length distributions, missing values)
- [ ] Initial visualizations (histograms, bar plots over time)
- [ ] Save processed subset to `/data/processed/` (for later parts)

---

## âœï¸ Section 3 â€” Part 2 (20 pts): Simple Text Processing

- [ ] Load spaCy model (`en_core_web_sm` or `md`)
- [ ] Clean text (lowercase, remove punctuation, stop words)
- [ ] Explain token vs lemma vs stop word vs punctuation
- [ ] Perform lemmatization (prefer lemmas for analysis)
- [ ] Compute frequency tables (most common lemmas)
- [ ] Apply TF-IDF vectorization (`sklearn.TfidfVectorizer`)
- [ ] Save vectorized matrix to `/data/vectorized/`

---

## ðŸ§  Section 4 â€” Part 3 (20 pts): Advanced Topic Modeling

- [ ] Build LDA model with `gensim` (on TF-IDF or BoW)
- [ ] Visualize LDA topics (top words, bar plots, pyLDAvis optional)
- [ ] Build BERTopic model (using BERT embeddings)
- [ ] Extract topics and clusters from BERTopic
- [ ] Compare LDA vs BERTopic (semantic vs bag-of-words)
- [ ] Save plots and topic figures to `/outputs/`
- [ ] Write markdown discussion on language evolution over time

---

## ðŸŒŸ Section 5 â€” Part 4 (Optional +7 pts): Choose Your Own Adventure

- [ ] Design extension (e.g., sentiment, NER network, LLM comparison)
- [ ] Document rationale and methods
- [ ] Include results in `nlp-P04.ipynb`

---

## ðŸ§± Section 6 â€” Reproducibility & Collaboration (Integrated)

- [ ] All notebooks run top-to-bottom without errors
- [ ] Figures auto-save to `/outputs/` with consistent names
- [ ] Use relative paths (no absolute system paths)
- [ ] Environment file recreates dependencies on DataHub
- [ ] Git history shows collaboration from all members
- [ ] Commit messages are clear and atomic (e.g., `feat: add bertopic plot`)
- [ ] AI usage logged in `ai_documentation.txt`
- [ ] Contribution statement authentic and complete

---

## ðŸ§ª Section 7 â€” Final Pre-Submission Validation

- [ ] Fresh clone â†’ `make all` runs cleanly
- [ ] MyST site builds successfully and renders on GitHub Pages
- [ ] Binder link launches and executes notebooks
- [ ] No large files (> 100 MB) tracked in Git history
- [ ] `README.md` includes team names, dependencies, run steps
- [ ] Push final version to `main` by **Nov 21 11:59 PM PT**

---

### ðŸ’¡ Tip
Save this as `PROJECT_CHECKLIST.md` in the repo root.  
During meetings, check off items directly (âœ…) or link each section to GitHub Issues for tracking progress.

---

